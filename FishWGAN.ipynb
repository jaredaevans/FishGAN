{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fish Generator\n",
    "\n",
    "The goal here is to make a GAN that can produce tropical looking fake fish\n",
    "\n",
    "Going to attempt to grow the network progressively a la https://arxiv.org/pdf/1710.10196.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fish_gan4_back_ep40.h5', 'fish_gan4_back_ep20.h5', '.DS_Store', 'GrabbedFromWeb', 'Loss', 'fish_gan_3.h5', 'FakeFish', 'fish_gan_2.h5', 'plot_line_plot_loss.png', 'ProcessedFish', 'Fish_discriminator_1.h5', 'h5s', 'FishGAN.ipynb', 'FishIMAGES.ipynb', 'FishGAN', 'GIFs', 'fish_gan_1.h5', 'fish_gan_5.h5', 'CustomFish', 'NewLabeledFish', 'FishWGAN.ipynb', '.ipynb_checkpoints', 'fish_gan6_back_ep20.h5', 'Fish_generator_1.h5', 'fish_gan6_back_ep40.h5', 'fish_gan2_back_ep120.h5', 'GrabbedFromWebBack']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np  \n",
    "import pandas as pd \n",
    "\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import shutil\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from scipy import linalg\n",
    "\n",
    "import xml.etree.ElementTree as ET \n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "from IPython import display\n",
    "from IPython.display import Image as IpyImage\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import UpSampling2D\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Add\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "\n",
    "from tensorflow.keras.models import save_model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "print(os.listdir(\"./\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FishDIR='./ProcessedFish/'\n",
    "FishDIR='./CustomFish/'\n",
    "GenFishDIR='./FakeFish/'\n",
    "GIFDIR = \"./GIFs/\"\n",
    "h5DIR = './h5s/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiple_images(images, n_cols=None):\n",
    "    n_cols = n_cols or len(images)\n",
    "    n_rows = (len(images) - 1) // n_cols + 1\n",
    "    if images.shape[-1] == 1:\n",
    "        images = np.squeeze(images, axis=-1)\n",
    "    plt.figure(figsize=(n_cols, n_rows))\n",
    "    for index, image in enumerate(images):\n",
    "        plt.subplot(n_rows, n_cols, index + 1)\n",
    "        plt.imshow(np.clip((image + 1.)/2.,0.,1.), cmap=\"binary\")\n",
    "        #plt.imshow(image, cmap=\"binary\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "def read_image(src, RESIZE=None):\n",
    "    img = cv2.imread(src)\n",
    "    if img is None:\n",
    "        print(src)\n",
    "        raise FileNotFoundError\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    if RESIZE is None:\n",
    "        return img\n",
    "    return img\n",
    "\n",
    "def write_image(img,filename):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(filename, img)\n",
    "\n",
    "customOs={'LeakyReLU': LeakyReLU}\n",
    "\n",
    "def constructGIF(ix,filename):\n",
    "    gif_path = GIFDIR+filename+\".gif\"\n",
    "    frames_path = GIFDIR+filename+\"_{i}.jpg\"\n",
    "    with imageio.get_writer(gif_path, mode='I') as writer:\n",
    "        for i in range(ix):\n",
    "            writer.append_data(imageio.imread(frames_path.format(i=i)))\n",
    "\n",
    "#IpyImage(filename=GIFDIR+modelname+\"_test.gif\")\n",
    "\n",
    "def plot_history(d1_hist, d2_hist, g_hist):\n",
    "    plt.plot(d1_hist, label='crit_real')\n",
    "    plt.plot(d2_hist, label='crit_fake')\n",
    "    plt.plot(g_hist, label='gen')\n",
    "    plt.legend()\n",
    "    #plt.savefig('./plot_line_plot_loss.png')\n",
    "    #plt.close()\n",
    "    \n",
    "def plot_history(d_hist, g_hist):\n",
    "    plt.plot(d_hist, label='crit')\n",
    "    plt.plot(g_hist, label='gen')\n",
    "    plt.legend()\n",
    "    #plt.savefig('./plot_line_plot_loss.png')\n",
    "    #plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turns all arguments to to float32\n",
    "def fp32(*values):\n",
    "    if len(values) == 1 and isinstance(values[0], tuple):\n",
    "        values = values[0]\n",
    "    values = tuple(tf.cast(v, tf.float32) for v in values)\n",
    "    return values if len(values) >= 2 else values[0]\n",
    "\n",
    "# from 1710.10196 use mini batch standard dev on discriminator output (only needs to be done once)\n",
    "# need to combine layers to fade out, inherit Add class\n",
    "# fade parameter is updated in WGANGP class\n",
    "class AddWithFade(Add):\n",
    "    def __init__(self, fade=0.0, **kwargs):\n",
    "        super(AddWithFade, self).__init__(**kwargs)\n",
    "        # fade will increase linearly from 0-1 \n",
    "        self.fade = K.variable(fade, name='fade_param')\n",
    " \n",
    "    def _merge_function(self, inputs):\n",
    "        assert (len(inputs) == 2)\n",
    "        #input[0] = lower res layer, input[1] = higher res layer \n",
    "        output = ((1. - self.fade) * inputs[0]) + (self.fade * inputs[1])\n",
    "        return output\n",
    "\n",
    "# update the fade parameter in the model\n",
    "def update_fade(model,newfade):\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, AddWithFade):\n",
    "            K.set_value(layer.fade,newfade)\n",
    "\n",
    "# implementation of wasserstein loss with gradient penalty\n",
    "# Useful information: https://medium.com/@jonathan_hui/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490\n",
    "# keras implementation: https://keras.io/examples/generative/wgan_gp/\n",
    "class WGANGP(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        discriminator,\n",
    "        generator,\n",
    "        latent_dim,\n",
    "        discriminator_steps=1,\n",
    "        gp_weight=10.0,\n",
    "        nMaxFade = 8000\n",
    "    ):\n",
    "        super(WGANGP, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.d_steps = discriminator_steps\n",
    "        self.gp_weight = gp_weight\n",
    "        self.nMaxFade = nMaxFade\n",
    "        self.fade = 0.0\n",
    "        self.nRunFade = 0\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
    "        super(WGANGP, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "\n",
    "    def gradient_penalty(self, batch_size, real_images, fake_images):\n",
    "        \"\"\" Calculates the gradient penalty.\n",
    "\n",
    "        This loss is calculated on an interpolated image\n",
    "        and added to the discriminator loss.\n",
    "        \"\"\"\n",
    "        # get the interplated image\n",
    "        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "        diff = fake_images - real_images\n",
    "        interpolated = real_images + alpha * diff\n",
    "\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            # 1. Get the discriminator output for this interpolated image.\n",
    "            pred = self.discriminator(interpolated, training=True)\n",
    "\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "        # 3. Calcuate the norm of the gradients\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp\n",
    "\n",
    "    def train_step(self, real_images, withFade = True):\n",
    "        if isinstance(real_images, tuple):\n",
    "            real_images = real_images[0]\n",
    "\n",
    "        # Get the batch size\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        \n",
    "        # update the fade is model is being run with fade\n",
    "        if withFade:\n",
    "            if self.nRunFade == 0:\n",
    "                self.nRunFade += int(batch_size/2)\n",
    "            else:\n",
    "                self.nRunFade += batch_size\n",
    "            self.fade = min(self.nRunFade,self.nMaxFade)/self.nMaxFade\n",
    "            update_fade(self.generator,self.fade)\n",
    "            update_fade(self.discriminator,self.fade)\n",
    "\n",
    "        # For each batch, we are going to perform the\n",
    "        # following steps as laid out in the original paper.\n",
    "        # 1. Train the generator and get the generator loss\n",
    "        # 2. Train the discriminator and get the discriminator loss\n",
    "        # 3. Calculate the gradient penalty\n",
    "        # 4. Multiply this gradient penalty with a constant weight factor\n",
    "        # 5. Add gradient penalty to the discriminator loss\n",
    "        # 6. Return generator and discriminator losses as a loss dictionary.\n",
    "\n",
    "        # Train discriminator first. The original paper recommends training\n",
    "        # the discriminator for `x` more steps (typically 5) as compared to\n",
    "        # one step of the generator. Here we will train it for 3 extra steps\n",
    "        # as compared to 5 to reduce the training time.\n",
    "        for i in range(self.d_steps):\n",
    "            # Get the latent vector\n",
    "            random_latent_vectors = tf.random.normal(\n",
    "                shape=(batch_size, self.latent_dim)\n",
    "            )\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Generate fake images from the latent vector\n",
    "                fake_images = self.generator(random_latent_vectors, training=True)\n",
    "                # Get the logits for the fake images\n",
    "                fake_logits = self.discriminator(fake_images, training=True)\n",
    "                # Get the logits for real images\n",
    "                real_logits = self.discriminator(real_images, training=True)\n",
    "\n",
    "                # Calculate discriminator loss using fake and real logits\n",
    "                d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n",
    "                # Calculate the gradient penalty\n",
    "                gp = self.gradient_penalty(batch_size, real_images, fake_images)\n",
    "                # Add the gradient penalty to the original discriminator loss\n",
    "                d_loss = d_cost + gp * self.gp_weight\n",
    "\n",
    "            # Get the gradients w.r.t the discriminator loss\n",
    "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            # Update the weights of the discriminator using the discriminator optimizer\n",
    "            self.d_optimizer.apply_gradients(\n",
    "                zip(d_gradient, self.discriminator.trainable_variables)\n",
    "            )\n",
    "\n",
    "        # Train the generator now.\n",
    "        # Get the latent vector\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate fake images using the generator\n",
    "            generated_images = self.generator(random_latent_vectors, training=True)\n",
    "            # Get the discriminator logits for fake images\n",
    "            gen_img_logits = self.discriminator(generated_images, training=True)\n",
    "            # Calculate the generator loss\n",
    "            g_loss = self.g_loss_fn(gen_img_logits)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this has the generator creating realistic deviations across batches\n",
    "# this is likely more important for faces, etc\n",
    "class MBStDev(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.smallParam = 1e-8 \n",
    "        super(MBStDev, self).__init__(**kwargs)\n",
    " \n",
    "    # perform the operation\n",
    "    def call(self, ins):\n",
    "        # mean value for each pixel across channels\n",
    "        pixMean = K.mean(ins, axis=0, keepdims=True)\n",
    "        # standard deviation across each pixel coord (small param regulates singularity)\n",
    "        stDev = K.sqrt(K.mean(K.square(ins - pixMean), axis=0, keepdims=True)+self.smallParam)\n",
    "        # mean standard deviation across each pixel coord\n",
    "        meanStDev = K.mean(stDev, keepdims=True)\n",
    "        # scale this up to be the size of one input feature map for each sample\n",
    "        shape = K.shape(ins)\n",
    "        outs = K.tile(meanStDev, (shape[0], shape[1], shape[2], 1))\n",
    "        # concatenate with the output\n",
    "        joinedInandOut = K.concatenate([ins, outs], axis=-1)\n",
    "        return joinedInandOut\n",
    " \n",
    "    # corrects the output shape to match the joint values\n",
    "    def correct_output_shape(self, input_shape):\n",
    "        # create a copy of the input shape as a list\n",
    "        input_shape = list(input_shape)\n",
    "        # add one to the channel dimension (assume channels-last)\n",
    "        input_shape[-1] += 1\n",
    "        # convert list to a tuple\n",
    "        return tuple(input_shape)\n",
    "\n",
    "# from 1710.10196 - use pixel normalization, a variant of \"local response normalization\"\n",
    "# Used to \"disallow the scenario where the magnitudes in the generator and discriminator spiral out \n",
    "# of control as a result of competition\" - apply BEFORE activation function in generator only\n",
    "class PixelNorm(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.smallParam = 1e-8 \n",
    "        super(PixelNorm, self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self, ins):\n",
    "        # -1 is over the filters\n",
    "        sqPixMean = K.mean(ins**2 + self.smallParam, axis=-1, keepdims=True)\n",
    "        return ins / K.sqrt(sqPixMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://keras.io/examples/generative/wgan_gp/\n",
    "# Define the loss functions to be used for discrimiator\n",
    "# This should be (fake_loss - real_loss)\n",
    "# We will add the gradient penalty later to this loss function\n",
    "def critic_loss(real_img, fake_img):\n",
    "    real_loss = tf.reduce_mean(real_img)\n",
    "    fake_loss = tf.reduce_mean(fake_img)\n",
    "    # \n",
    "    # this is a simple implementation of the drift loss\n",
    "    epsilonDrift = 0.001\n",
    "    epsilonLoss = epsilonDrift * tf.reduce_mean(tf.nn.l2_loss(real_img))\n",
    "    return fake_loss - real_loss + epsilonLoss\n",
    "\n",
    "# Define the loss functions to be used for generator\n",
    "def generator_loss(fake_img):\n",
    "    return -tf.reduce_mean(fake_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leveldict = {4: {'filters': 256},\n",
    "             8: {'filters': 256},\n",
    "            16: {'filters': 256},\n",
    "            32: {'filters': 256},\n",
    "            64: {'filters': 128},\n",
    "            128: {'filters': 64}}\n",
    "\n",
    "optimizercrit=Adam(lr=0.001, beta_1=0, beta_2=0.99, epsilon=1e-8)\n",
    "optimizergen=Adam(lr=0.001, beta_1=0, beta_2=0.99, epsilon=1e-8)\n",
    "\n",
    "# note: have not yet included section 4.1 EQUALIZED LEARNING RATE as implementing this is\n",
    "#    a) something I am not certain how to do and b) potentially a solution very prone to errors\n",
    "#    where I am doing something that would be replaced for that I've added a #4.1\n",
    "\n",
    "# take from a model that criticises an nxnx3 figure to one that criticizes a 2nx2nx3 figure\n",
    "def add_critic_level(old_c_model, new_level=8, nSkip=3,nSkipPassby=1,nColors=3):\n",
    "    thisleveldict = leveldict[new_level]\n",
    "    filters = thisleveldict['filters']\n",
    "    conv2filters = leveldict[new_level/2]['filters']\n",
    "    # #4.1 weight initialization and maxnorm constraint\n",
    "    kinit = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "    kconstraint = None \n",
    "    \n",
    "    #define new input layer\n",
    "    layer_in = Input(shape=(new_level, new_level, nColors,)) \n",
    "    # new round start passby here\n",
    "    conv_0 = Conv2D(filters, (1,1), padding=\"SAME\", kernel_initializer=kinit, kernel_constraint=kconstraint)(layer_in)\n",
    "    act_0 = LeakyReLU(alpha=0.2)(conv_0)\n",
    "    # first layer set (newround start main model here)\n",
    "    conv_1 = Conv2D(filters, (3,3), padding=\"SAME\", kernel_initializer=kinit, kernel_constraint=kconstraint)(act_0)\n",
    "    act_1 = LeakyReLU(alpha=0.2)(conv_1)\n",
    "    # second layer set\n",
    "    conv_2 = Conv2D(conv2filters, (3,3), padding=\"SAME\", kernel_initializer=kinit, kernel_constraint=kconstraint)(act_1)\n",
    "    act_2 = LeakyReLU(alpha=0.2)(conv_2)\n",
    "    newcriticlayers = AveragePooling2D()(act_2)\n",
    "    critic = newcriticlayers\n",
    "    # append the earlier model\n",
    "    for i in range(nSkip,len(old_c_model.layers)):\n",
    "        critic = old_c_model.layers[i](critic)\n",
    "    criticModel = Model(layer_in, critic)\n",
    "    criticModel.compile(loss=critic_loss, optimizer=optimizercrit)\n",
    "    \n",
    "    # define passby model, first downsample\n",
    "    pathtofade = AveragePooling2D()(layer_in)\n",
    "    # note: including conv_0, and act_0 from previous layer\n",
    "    for i in range(nSkipPassby,nSkip):\n",
    "        pathtofade = old_c_model.layers[i](pathtofade)\n",
    "    critic_fade = AddWithFade()([pathtofade,newcriticlayers])\n",
    "    for i in range(nSkip,len(old_c_model.layers)):\n",
    "        critic_fade = old_c_model.layers[i](critic_fade)\n",
    "    criticFadeOut = Model(layer_in, critic_fade)\n",
    "    criticFadeOut.compile(loss=critic_loss, optimizer=optimizercrit)\n",
    "    return [criticModel, criticFadeOut]\n",
    "\n",
    "# take from a model that generates an nxnx3 figure to one that generates a 2nx2nx3 figure\n",
    "def add_generator_level(old_g_model, new_level=8, nDrop=3,nDropPassby=1,nColors=3):\n",
    "    thisleveldict = leveldict[new_level]\n",
    "    filters = thisleveldict['filters']\n",
    "    # define input shape\n",
    "    #new_model_in_shape =  [ new_level, new_level, n_input_layers]\n",
    "    # #4.1 weight initialization and maxnorm constraint\n",
    "    kinit = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "    kconstraint = None \n",
    "    \n",
    "    # get input\n",
    "    layer_in = old_g_model.input\n",
    "    # remove final layer\n",
    "    newendofold = old_g_model.layers[-2].output\n",
    "    sizeaugment = UpSampling2D()(newendofold)\n",
    "    \n",
    "    # first layer set\n",
    "    conv_1 = Conv2D(filters, (3,3), padding=\"SAME\", kernel_initializer=kinit, kernel_constraint=kconstraint)(sizeaugment)\n",
    "    pixnorm_1 = PixelNorm()(conv_1)\n",
    "    act_1 = LeakyReLU(alpha=0.2)(pixnorm_1)\n",
    "    # second layer set\n",
    "    conv_2 = Conv2D(filters, (3,3), padding=\"SAME\", kernel_initializer=kinit, kernel_constraint=kconstraint)(act_1)\n",
    "    pixnorm_2 = PixelNorm()(conv_2)\n",
    "    act_2 = LeakyReLU(alpha=0.2)(pixnorm_2)\n",
    "    # save for combo below\n",
    "    conv_out = Conv2D(nColors, (1,1), padding=\"SAME\", kernel_initializer=kinit, kernel_constraint=kconstraint)(act_2)\n",
    "    genModel = Model(layer_in, conv_out)\n",
    "    \n",
    "    # define passby model\n",
    "    old_end = old_g_model.layers[-1].output\n",
    "    sizeaugment_fade = UpSampling2D()(old_end)\n",
    "    conv_out_fade = AddWithFade()([sizeaugment_fade,conv_out])\n",
    "    \n",
    "    genModelFade = Model(layer_in, conv_out_fade)\n",
    "    \n",
    "    return [genModel, genModelFade]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nLevels = number of pixelation levels (4,8,16,32,64,128) \n",
    "def create_critics(nColors=3, initialSize = 4, nLevels = 6):\n",
    "    # first create the lowest level critic\n",
    "    thisleveldict = leveldict[initialSize]\n",
    "    filters = thisleveldict['filters']\n",
    "    # #4.1 weight initialization and maxnorm constraint\n",
    "    kinit = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "    kconstraint = None \n",
    "    #kconstraint = keras.constraints.max_norm(1.0)\n",
    "    \n",
    "    #define new input layer\n",
    "    layer_in = Input(shape=(initialSize, initialSize, nColors,)) \n",
    "    # new round start passby here\n",
    "    # define new input processing layer\n",
    "    conv_0 = Conv2D(filters, (1,1), padding='SAME', kernel_initializer=kinit, kernel_constraint=kconstraint)(layer_in)\n",
    "    act_0 = LeakyReLU(alpha=0.2)(conv_0)\n",
    "    # first layer set (newround start main model here)\n",
    "    # apply minibatch standard deviation\n",
    "    miniBDev = MBStDev()(act_0)\n",
    "    conv_1 = Conv2D(filters, (3,3), padding=\"SAME\", kernel_initializer=kinit, kernel_constraint=kconstraint)(miniBDev)\n",
    "    act_1 = LeakyReLU(alpha=0.2)(conv_1)\n",
    "    # second layer set (the end is a 4x4 convolution)\n",
    "    conv_2 = Conv2D(filters, (4,4), padding=\"SAME\", kernel_initializer=kinit, kernel_constraint=kconstraint)(act_1)\n",
    "    act_2 = LeakyReLU(alpha=0.2)(conv_2)\n",
    "    dense_out = Flatten()(act_2)\n",
    "    out_classifier = Dense(1)(dense_out)\n",
    "    \n",
    "    # define and compile model\n",
    "    initial_critic = Model(layer_in,out_classifier)\n",
    "    initial_critic.compile(loss=critic_loss, optimizer=optimizercrit)\n",
    "    \n",
    "    # collect all models\n",
    "    modellist = [[initial_critic,initial_critic]]\n",
    "    curlevel = 4\n",
    "    for i in range(1,nLevels):\n",
    "        curlevel = curlevel * 2\n",
    "        # modellist[-1][0] corresponds to the version with no fade\n",
    "        newmodels = add_critic_level(modellist[-1][0], new_level=curlevel)\n",
    "        modellist.append(newmodels)\n",
    "    return modellist\n",
    "\n",
    "\n",
    "# nLevels = number of pixelation levels (4,8,16,32,64,128) \n",
    "def create_gens(nInputs =128, nColors=3, initialSize = 4, nLevels = 6):\n",
    "    # first create the lowest level critic\n",
    "    thisleveldict = leveldict[initialSize]\n",
    "    filters = thisleveldict['filters']\n",
    "    # #4.1 weight initialization and maxnorm constraint\n",
    "    kinit = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "    kconstraint = None \n",
    "    \n",
    "    #define new input layer\n",
    "    layer_in = Input(shape=(nInputs,))\n",
    "    dense_0 = Dense(nInputs*initialSize*initialSize, kernel_initializer=kinit, kernel_constraint=kconstraint)(layer_in)\n",
    "    reshape_0 = Reshape((initialSize, initialSize, nInputs))(dense_0)\n",
    "    #may want to add activiation functions\n",
    "    # first layer set (start with 4x4)\n",
    "    conv_1 = Conv2D(filters, (4,4), padding=\"SAME\", kernel_initializer=kinit, kernel_constraint=kconstraint)(reshape_0)\n",
    "    pixnorm_1 = PixelNorm()(conv_1)\n",
    "    act_1 = LeakyReLU(alpha=0.2)(pixnorm_1)\n",
    "    # second layer set\n",
    "    conv_2 = Conv2D(filters, (3,3), padding=\"SAME\", kernel_initializer=kinit, kernel_constraint=kconstraint)(act_1)\n",
    "    pixnorm_2 = PixelNorm()(conv_2)\n",
    "    act_2 = LeakyReLU(alpha=0.2)(pixnorm_2)\n",
    "    # save for combo below\n",
    "    conv_out = Conv2D(nColors, (1,1), padding=\"SAME\", kernel_initializer=kinit, kernel_constraint=kconstraint)(act_2)\n",
    "    genModel = Model(layer_in, conv_out)\n",
    "    \n",
    "        \n",
    "    # collect all models\n",
    "    modellist = [[genModel,genModel]]\n",
    "    curlevel = 4\n",
    "    for i in range(1,nLevels):\n",
    "        curlevel = curlevel * 2\n",
    "        # modellist[-1][0] corresponds to the version with no fade\n",
    "        newmodels = add_generator_level(modellist[-1][0], new_level=curlevel)\n",
    "        modellist.append(newmodels)\n",
    "    return modellist\n",
    "\n",
    "def create_gans(gens,crits,nInputs = 128):\n",
    "    ganlist = []\n",
    "    assert len(gens)==len(crits), \"Generators and Discriminators created with different lengths\"\n",
    "    for i in range(len(gens)):\n",
    "        # compile standard\n",
    "        wgan1 = WGANGP(discriminator=crits[i][0],generator=gens[i][0],latent_dim=nInputs,discriminator_steps=1)\n",
    "        wgan1.compile(d_optimizer=optimizercrit,g_optimizer=optimizergen,g_loss_fn=generator_loss,d_loss_fn=critic_loss)\n",
    "        # compile fade\n",
    "        wgan2 = WGANGP(discriminator=crits[i][1],generator=gens[i][1],latent_dim=nInputs,discriminator_steps=1)\n",
    "        wgan2.compile(d_optimizer=optimizercrit,g_optimizer=optimizergen,g_loss_fn=generator_loss,d_loss_fn=critic_loss)\n",
    "        # add to gan list\n",
    "        ganlist.append([wgan1,wgan2])\n",
    "    return ganlist\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELR version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this kernel implements equalized learning rate\n",
    "# adapted from https://github.com/preritj/progressive_growing_of_GANs/blob/master/net.py\n",
    "class Conv2DELR(Conv2D):\n",
    "    \"\"\"\n",
    "    Standard Conv2D layer but includes learning rate equilization\n",
    "    at runtime as per Karras et al. 2017.\n",
    "\n",
    "    Inherits Conv2D layer and overrides the call method, following\n",
    "    https://github.com/keras-team/keras/blob/master/keras/layers/convolutional.py\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        if 'kernel_initializer' in kwargs:\n",
    "            raise Exception(\"Cannot override kernel_initializer\")\n",
    "        super().__init__(*args, kernel_initializer=tf.keras.initializers.RandomNormal(stddev=1.), **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        # The number of inputs\n",
    "        n = np.product([int(val) for val in input_shape[1:]])\n",
    "        # He initialisation constant\n",
    "        self.c = np.sqrt(2/n)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if self.rank == 2:\n",
    "            outputs = K.conv2d(\n",
    "                inputs,\n",
    "                self.kernel*self.c, # scale kernel\n",
    "                strides=self.strides,\n",
    "                padding=self.padding,\n",
    "                data_format=self.data_format,\n",
    "                dilation_rate=self.dilation_rate)\n",
    "\n",
    "        if self.use_bias:\n",
    "            outputs = K.bias_add(\n",
    "                outputs,\n",
    "                self.bias,\n",
    "                data_format=self.data_format)\n",
    "\n",
    "        if self.activation is not None:\n",
    "            return self.activation(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "leveldict = {4: {'filters': 256},\n",
    "             8: {'filters': 256},\n",
    "            16: {'filters': 256},\n",
    "            32: {'filters': 256},\n",
    "            64: {'filters': 128},\n",
    "            128: {'filters': 64}}\n",
    "\n",
    "optimizercrit=Adam(lr=0.001, beta_1=0, beta_2=0.99, epsilon=1e-8)\n",
    "optimizergen=Adam(lr=0.001, beta_1=0, beta_2=0.99, epsilon=1e-8)\n",
    "\n",
    "# note: have not yet included section 4.1 EQUALIZED LEARNING RATE as implementing this is\n",
    "#    a) something I am not certain how to do and b) potentially a solution very prone to errors\n",
    "#    where I am doing something that would be replaced for that I've added a #4.1\n",
    "\n",
    "# take from a model that criticises an nxnx3 figure to one that criticizes a 2nx2nx3 figure\n",
    "def add_critic_level(old_c_model, new_level=8, nSkip=3,nSkipPassby=1,nColors=3):\n",
    "    thisleveldict = leveldict[new_level]\n",
    "    filters = thisleveldict['filters']\n",
    "    conv2filters = leveldict[new_level/2]['filters']\n",
    "    \n",
    "    #define new input layer\n",
    "    layer_in = Input(shape=(new_level, new_level, nColors,)) \n",
    "    # new round start passby here\n",
    "    conv_0 = Conv2DELR(filters, (3,3), padding=\"SAME\")(layer_in)\n",
    "    act_0 = LeakyReLU(alpha=0.2)(conv_0)\n",
    "    # first layer set (newround start main model here)\n",
    "    conv_1 = Conv2DELR(filters, (3,3), padding=\"SAME\")(act_0)\n",
    "    act_1 = LeakyReLU(alpha=0.2)(conv_1)\n",
    "    # second layer set\n",
    "    conv_2 = Conv2DELR(conv2filters, (3,3), padding=\"SAME\")(act_1)\n",
    "    act_2 = LeakyReLU(alpha=0.2)(conv_2)\n",
    "    newcriticlayers = AveragePooling2D()(act_2)\n",
    "    critic = newcriticlayers\n",
    "    # append the earlier model\n",
    "    for i in range(nSkip,len(old_c_model.layers)):\n",
    "        critic = old_c_model.layers[i](critic)\n",
    "    criticModel = Model(layer_in, critic)\n",
    "    \n",
    "    # define passby model, first downsample\n",
    "    pathtofade = AveragePooling2D()(layer_in)\n",
    "    # note: including conv_0, and act_0 from previous layer\n",
    "    for i in range(nSkipPassby,nSkip):\n",
    "        pathtofade = old_c_model.layers[i](pathtofade)\n",
    "    critic_fade = AddWithFade()([pathtofade,newcriticlayers])\n",
    "    for i in range(nSkip,len(old_c_model.layers)):\n",
    "        critic_fade = old_c_model.layers[i](critic_fade)\n",
    "    criticFadeOut = Model(layer_in, critic_fade)\n",
    "    return [criticModel, criticFadeOut]\n",
    "\n",
    "# take from a model that generates an nxnx3 figure to one that generates a 2nx2nx3 figure\n",
    "def add_generator_level(old_g_model, new_level=8, nDrop=3,nDropPassby=1,nColors=3):\n",
    "    thisleveldict = leveldict[new_level]\n",
    "    filters = thisleveldict['filters']\n",
    "    \n",
    "    # get input\n",
    "    layer_in = old_g_model.input\n",
    "    # remove final layer\n",
    "    newendofold = old_g_model.layers[-2].output\n",
    "    sizeaugment = UpSampling2D()(newendofold)\n",
    "    \n",
    "    # first layer set\n",
    "    conv_1 = Conv2DELR(filters, (3,3), padding=\"SAME\")(sizeaugment)\n",
    "    pixnorm_1 = PixelNorm()(conv_1)\n",
    "    act_1 = LeakyReLU(alpha=0.2)(pixnorm_1)\n",
    "    # second layer set\n",
    "    conv_2 = Conv2DELR(filters, (3,3), padding=\"SAME\")(act_1)\n",
    "    pixnorm_2 = PixelNorm()(conv_2)\n",
    "    act_2 = LeakyReLU(alpha=0.2)(pixnorm_2)\n",
    "    # save for combo below\n",
    "    conv_out = Conv2DELR(nColors, (1,1), padding=\"SAME\")(act_2)\n",
    "    genModel = Model(layer_in, conv_out)\n",
    "    \n",
    "    # define passby model\n",
    "    old_end = old_g_model.layers[-1].output\n",
    "    sizeaugment_fade = UpSampling2D()(old_end)\n",
    "    conv_out_fade = AddWithFade()([sizeaugment_fade,conv_out])\n",
    "    \n",
    "    genModelFade = Model(layer_in, conv_out_fade)\n",
    "    \n",
    "    return [genModel, genModelFade]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nLevels = number of pixelation levels (4,8,16,32,64,128) \n",
    "def create_critics(nColors=3, initialSize = 4, nLevels = 6):\n",
    "    # first create the lowest level critic\n",
    "    thisleveldict = leveldict[initialSize]\n",
    "    filters = thisleveldict['filters']\n",
    "    \n",
    "    #define new input layer\n",
    "    layer_in = Input(shape=(initialSize, initialSize, nColors,)) \n",
    "    # new round start passby here\n",
    "    # define new input processing layer\n",
    "    conv_0 = Conv2DELR(filters, (1,1), padding=\"SAME\")(layer_in)\n",
    "    act_0 = LeakyReLU(alpha=0.2)(conv_0)\n",
    "    # first layer set (newround start main model here)\n",
    "    # apply minibatch standard deviation\n",
    "    miniBDev = MBStDev()(act_0)\n",
    "    conv_1 = Conv2DELR(filters, (3,3), padding=\"SAME\")(miniBDev)\n",
    "    act_1 = LeakyReLU(alpha=0.2)(conv_1)\n",
    "    # second layer set (the end is a 4x4 convolution)\n",
    "    conv_2 = Conv2DELR(filters, (4,4), padding=\"SAME\")(act_1)\n",
    "    act_2 = LeakyReLU(alpha=0.2)(conv_2)\n",
    "    dense_out = Flatten()(act_2)\n",
    "    out_classifier = Dense(1)(dense_out)\n",
    "    \n",
    "    # define and compile model\n",
    "    initial_critic = Model(layer_in,out_classifier)\n",
    "    \n",
    "    # collect all models\n",
    "    modellist = [[initial_critic,initial_critic]]\n",
    "    curlevel = 4\n",
    "    for i in range(1,nLevels):\n",
    "        curlevel = curlevel * 2\n",
    "        # modellist[-1][0] corresponds to the version with no fade\n",
    "        newmodels = add_critic_level(modellist[-1][0], new_level=curlevel)\n",
    "        modellist.append(newmodels)\n",
    "    return modellist\n",
    "\n",
    "\n",
    "# nLevels = number of pixelation levels (4,8,16,32,64,128) \n",
    "def create_gens(nInputs =128, nColors=3, initialSize = 4, nLevels = 6):\n",
    "    # first create the lowest level critic\n",
    "    thisleveldict = leveldict[initialSize]\n",
    "    filters = thisleveldict['filters']\n",
    "    # #4.1 weight initialization and maxnorm constraint\n",
    "    kinit = tf.keras.initializers.RandomNormal(stddev=1.)\n",
    "    \n",
    "    #define new input layer\n",
    "    layer_in = Input(shape=(nInputs,))\n",
    "    dense_0 = Dense(nInputs*initialSize*initialSize, kernel_initializer=kinit)(layer_in)\n",
    "    reshape_0 = Reshape((initialSize, initialSize, nInputs))(dense_0)\n",
    "    #may want to add activiation functions\n",
    "    # first layer set (start with 4x4)\n",
    "    conv_1 = Conv2DELR(filters, (4,4), padding=\"SAME\")(reshape_0)\n",
    "    pixnorm_1 = PixelNorm()(conv_1)\n",
    "    act_1 = LeakyReLU(alpha=0.2)(pixnorm_1)\n",
    "    # second layer set \n",
    "    conv_2 = Conv2DELR(filters, (3,3), padding=\"SAME\")(act_1)\n",
    "    pixnorm_2 = PixelNorm()(conv_2)\n",
    "    act_2 = LeakyReLU(alpha=0.2)(pixnorm_2)\n",
    "    # save for combo below\n",
    "    conv_out = Conv2DELR(nColors, (1,1), padding=\"SAME\")(act_2)\n",
    "    genModel = Model(layer_in, conv_out)\n",
    "    \n",
    "        \n",
    "    # collect all models\n",
    "    modellist = [[genModel,genModel]]\n",
    "    curlevel = 4\n",
    "    for i in range(1,nLevels):\n",
    "        curlevel = curlevel * 2\n",
    "        # modellist[-1][0] corresponds to the version with no fade\n",
    "        newmodels = add_generator_level(modellist[-1][0], new_level=curlevel)\n",
    "        modellist.append(newmodels)\n",
    "    return modellist\n",
    "\n",
    "def create_gans(gens,crits,nInputs = 128):\n",
    "    ganlist = []\n",
    "    assert len(gens)==len(crits), \"Generators and Discriminators created with different lengths\"\n",
    "    for i in range(len(gens)):\n",
    "        # compile standard\n",
    "        wgan1 = WGANGP(discriminator=crits[i][0],generator=gens[i][0],latent_dim=nInputs,discriminator_steps=1)\n",
    "        wgan1.compile(d_optimizer=optimizercrit,g_optimizer=optimizergen,g_loss_fn=generator_loss,d_loss_fn=critic_loss)\n",
    "        # compile fade\n",
    "        wgan2 = WGANGP(discriminator=crits[i][1],generator=gens[i][1],latent_dim=nInputs,discriminator_steps=1)\n",
    "        wgan2.compile(d_optimizer=optimizercrit,g_optimizer=optimizergen,g_loss_fn=generator_loss,d_loss_fn=critic_loss)\n",
    "        # add to gan list\n",
    "        ganlist.append([wgan1,wgan2])\n",
    "    return ganlist\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image processing\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=25,\n",
    "        width_shift_range=0.05,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.05,\n",
    "        zoom_range=[0.95,1.1],\n",
    "        brightness_range=[0.8,1.3],\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "# adjust the saturation of the image\n",
    "def colorSaturationAdjust(img, satRange=[0.6,1.4]):\n",
    "    rSat = random.triangular(satRange[0],satRange[1])\n",
    "    gSat = random.triangular(satRange[0],satRange[1])\n",
    "    bSat = random.triangular(satRange[0],satRange[1])\n",
    "    satVar = [rSat, gSat, bSat]\n",
    "    return np.clip(img * satVar,0,1)\n",
    "\n",
    "def multiprocessingI(imglist):\n",
    "    imglistproc = []\n",
    "    for img in datagen.flow((imglist+1.)/2.000)[0]:\n",
    "        imglistproc.append(np.clip(2.*(colorSaturationAdjust(img/255.0)-0.5),-1.,1.))\n",
    "        #imglistproc.append(colorSaturationAdjust(img/255.0))\n",
    "    return np.asarray(imglistproc)\n",
    "\n",
    "def multiprocessingI(imglist,new_length):\n",
    "    imglistproc = []\n",
    "    for img in datagen.flow((imglist+1.)/2.000)[0]:\n",
    "        newimg = np.clip(2.*(colorSaturationAdjust(img/255.0)-0.5),-1.,1.)\n",
    "        imglistproc.append(cv2.resize(newimg, (new_length,new_length), interpolation = cv2.INTER_AREA))\n",
    "        #imglistproc.append(colorSaturationAdjust(img/255.0))\n",
    "    return np.asarray(imglistproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create models\n",
    "genlist = create_gens()\n",
    "criticlist = create_critics()\n",
    "ganlist = create_gans(genlist,criticlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "FishFiles = os.listdir(FishDIR)\n",
    "# reshape and rescale into a numpy array\n",
    "Xtrain = np.stack([ read_image(FishDIR+i) * np.float32(2. / 255.) - 1  for i in FishFiles ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 32 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_multiple_images(multiprocessingI(Xtrain[:32],4),8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for GIF\n",
    "batch_size = 32\n",
    "codings_size = 128\n",
    "fixednoise = tf.random.normal(shape=[batch_size, codings_size])\n",
    "imgsetforGIF = []\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(Xtrain)\n",
    "dataset = dataset.shuffle(1000)\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_loss_hist, g_loss_hist = list(), list()\n",
    "\n",
    "def train_wgan(wgan, dataset, batch_size, codings_size, image_size, n_epochs=10):\n",
    "    generator = wgan.generator\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, n_epochs))  \n",
    "        d_loss, g_loss = 0, 0\n",
    "        starttime = time.time()\n",
    "        for X_batch in dataset:\n",
    "            lossdict = wgan.train_step(multiprocessingI(X_batch,image_size))\n",
    "            d_loss += lossdict['d_loss']\n",
    "            g_loss += lossdict['g_loss']\n",
    "        if epoch % 3 == 0 or epoch+1 == n_epochs: \n",
    "            generated_images = generator(fixednoise)\n",
    "            imgsetforGIF.append(generated_images)\n",
    "            plot_multiple_images(generated_images, 8)\n",
    "            plt.show()                                        \n",
    "        d_loss_hist.append(d_loss)\n",
    "        g_loss_hist.append(g_loss)\n",
    "        print('Epoch took {time:.3f} seconds: d_loss={dl:.3f},  g_loss={gl:.3f}'.format(time=time.time() - starttime, dl = d_loss, gl = g_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer conv2delr_18 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcwAAADnCAYAAACTx2bHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAPvklEQVR4nO3c6Y/V533G4Rk2sxsYGMxi9sXGxgZswAYvSbzITp2UNm0Wu42aqC+cKmoqtZGqNpXyomqlSK1SNVWr9EWsJFUaR3G9xHHS2I5jGxtis3kBAmYziweGYYZtZoAZpn9AzJzvLVE5ra7rLR8ffg9zDjdHsp7mgYGBJgBgcEM+6AcAgP8LDCYAFBhMACgwmABQYDABoGDYYL+4+re/Gv0vtO+992b0m4+f2xL1bTO6ov74Pz7a3KhZuu6L0RmvPP1G9AzHXh8f9fPuGh71P3nsvxqe8eZZfxqdcWDe9ugZxo1YGvUX32mP+hf3frfhGdd94bPRGY89dyh6holnsp/L+CmDfrR+zfe2Pd3wjNetezg646QTPdEzjNiSvbeXzr8+6r++9TsNz/jg6j+Mzrh/SPh3wokZUX9uWPY+OfD2jxqece30pdEZp6zKPl8XjoyI+t6uo1H/3K5nGp5x4X1/EZ2xf2z0CE2jNr4X9cdGHo/69t0/fd8z+oYJAAUGEwAKDCYAFBhMACgwmABQYDABoMBgAkCBwQSAAoMJAAUGEwAKDCYAFAx64eXIQ1uiFxt38aWov/jSTVE/7+55UV9yvjfK159tjfqPTzsX9U/OyO7nrOhYuifqRx8eGvWHrtgU9fMeuC/qK9afnhD1/dePjvqTZ16I+jWLb436ipb9B6N+SNP5qO+bMyXqN1+R3T1b8ULnrqifN7As6kfNaov64+PnR33FK1PWRP2SE09H/eSR2TMvP3cy6itart0W9dvHZHfDzjq3IuonTFkY9ZfiGyYAFBhMACgwmABQYDABoMBgAkCBwQSAAoMJAAUGEwAKDCYAFBhMACgwmABQMOhdssMXtUcvNnVbdtfr7psuRP3yN7O7bSuGHeuP+jsP/zDqN9z5cNQv2rQz6isWj++L+p9OPR31953tiPr9ux+N+qamv2pYfGhHdt/mlnuz9+rUt45H/cjD70R9RWdL9u/bnu0jo37Z5OxO4Fc77oj6iqXLxkR9e0/2+vvfeT3q77oqu6O4YvmI56O+6+KHo35g+fao3zhpRNRXDN3RFfUrhxyO+lMnsr/TWk5cnu+GvmECQIHBBIACgwkABQYTAAoMJgAUGEwAKDCYAFBgMAGgwGACQIHBBIACgwkABYPeJXuuZ3b0YlOmZvcBLuq/MurPLDsa9RXTJ2Z3hK7/9D1Rv2RXdlft5K6BqK/onX0y6u/uORX1+86viPrzC7M7hCtOzLo9e4bN+6P+4IS/jPrehVujvmLyNWei/pXjg368f03/Tdk9rk3dy7K+oLWzO+pPHsnuv72q/4Go3zblWNRXHFgzJ+pnjmqL+u4j2T3JeydPjfqKIcNGR/2xmROj/uy0m6O+c8bwqL8U3zABoMBgAkCBwQSAAoMJAAUGEwAKDCYAFBhMACgwmABQYDABoMBgAkCBwQSAguaBgct/dykA/H/jGyYAFBhMACgwmABQYDABoMBgAkCBwQSAAoMJAAUGEwAKhg32i3d89t7oVoMze3ui3/y6CVOi/vzbzVH//X0/bPgffGLJH0VnfG3p8egZ7nx2TNQfuqs76n/+6FMNzzj7Y38WnfHK3s3RM5waOjPqW26eGPWb/vZfGp7x7gdvjM7YsWdR9Awnz1wb9SsXvhD133/8xYZn/Oz9D2fv1fXZM4xozc7YumRN1P/syS83POMff/Hz0Rl/uTn7fLX1nIr6phsnR/mxR/6h4Rk/9LE/iM541eH3omdoHz8i6ptOzY/y5zZ/o+EZf2/lg9EZf3ZkZPQMixdlfybXbBwV9d/ufux9z+gbJgAUGEwAKDCYAFBgMAGgwGACQIHBBIACgwkABQYTAAoMJgAUGEwAKBj0ary+Sdk1bRc7xkX9zp7xUT9i1p6orxi647GovzD57qhfv/Ri1N86anbUVywf+WTU77vigaif2XQ06vuGdUZ9xaiL87J++utRv/XClqi/7aZbo77il2e3Rf2quTOi/qfjs2vjdrRf/s/jpg27ov6T/dk1i4/NPBn1Ew+NjvqK7pPnov7AvOiWuab33squgRs7LHv9irMtXVG/sDX77rbqpfaob2u5PH/n+IYJAAUGEwAKDCYAFBhMACgwmABQYDABoMBgAkCBwQSAAoMJAAUGEwAKDCYAFAx6l+y0w5OjF+s9nN2D2js0u4P02vnZ81Q8/9D9UT9k2DtR394R3gO5d1/Ul56h+6aon77nO1H/kznXR/3Kr2X3fzZ9tXEycFVv9JLNT2c/l/mzeqK+a/PWqK+YPnNm1L89cm/Uj3ppUtQvveWJqG9q+reGRfeytugVX313TNRvOTIl6u8ccijqK3au6oj66x49HvUr+rL36v65R6K+4q227OfS3L856nc+tCjqt526L+ovxTdMACgwmABQYDABoMBgAkCBwQSAAoMJAAUGEwAKDCYAFBhMACgwmABQYDABoGDQu2S3Xtscvdiad7dE/fD5E6L+QO+0qK+4bUR2B+nUvuzfGG+duBD1p9bOjfqKte91Rv2mmS1Rv2rM0KhfdGt31Ff07Z8e9Yvv2BX1Q9qvifqzi05HfcWVY66M+p6+c1G/9o4zUf/0/C9EfcWnf7Uy6r8+sSvqh63L7mV969DaqK+4ZkH2c9ywanXUL9u8PeoPrcruZa1YOm1O1P9y74ioHz5kVNTfdXFn1F+Kb5gAUGAwAaDAYAJAgcEEgAKDCQAFBhMACgwmABQYTAAoMJgAUGAwAaDAYAJAQfPAwMAH/QwA8BvPN0wAKDCYAFBgMAGgwGACQIHBBIACgwkABQYTAAoMJgAUGEwAKBg22C9+6SP3RNcA/fepjug3PzfqI1G/5vrtUf/df/1xc6Nm2gN/Hp1xXt+B6Bn2nX8i6j88/stR/x+P/13DM9738O3RGc/9qC96hg2LVkX9Axd2R/0PXmr8c1y37lPRGY+9fjp6hplzsxuxeiZMjfqnnnqk4Rl/98E/iR6i85nno2c48slror6nbWzUv/vEdxue8c6lfxOd8dV566NnmLb7fNS3jloe9a9t+ueGZ/z8ilXRGV9fviB6hgXPHo76PUNPRP22vW82POOHb8j+Xm2a0BPlrb/KPl9HV2+L+heefOx9z+gbJgAUGEwAKDCYAFBgMAGgwGACQIHBBIACgwkABQYTAAoMJgAUGEwAKBj0aryjd8+MXuzCG+1Rv68v61cdGhH1FRcOvhr1p+Ycifp7e5dE/SudE6K+ov/goagffcOyqO89cjHq3/1f+GfaoYvZe2lT68io/62WcVE/9OV3or6id9vGqD/Ykr2XbvpWa9S/dOvRqK/omZddYfaF/aOj/uzV2fuk88LWqK/Y1Tkn6pe8nD3zjllDo37uyy1RX9E/dFPU9zZdH/WP93496idc/ETUX4pvmABQYDABoMBgAkCBwQSAAoMJAAUGEwAKDCYAFBhMACgwmABQYDABoMBgAkDBoHfJ/uLpN6IXa74q29/ZQ+ZF/feP7476/yw0rS1jo9c89myUN/3onjlR37sgu6u2YufMm6L+ml+9GPWfG1gV9e3nBqK+YmDi1VH/uR2vRP2GruzO0qlTxkd9xcCHOqJ+ye51Uf9Y24Wov7ljatRXdJ0cHvWvXDcx6q9uHxP1W8de/rudL96Qfcb37OyO+tbd2c/xmc/fEfUVc+dNivo5kw5G/eHzt0X9mvbLc++xb5gAUGAwAaDAYAJAgcEEgAKDCQAFBhMACgwmABQYTAAoMJgAUGAwAaDAYAJAwaB3yU76/RXRi123oTfqt53bE/UPdmf3TFbMWdMS9ctuWxn1b7Xti/olz2Z3olZM3rU36tvHrI36l6+bHvXXX/GTqK9YMXp21G+a3xn1a4Z0Rf17Hz0R9RWtwz4T9T37347629dmn98JMy//XbIXF4+M+r2Ts5/juPA+3sU/HhX1FdtX90f96jlLov7UjrNRv+DQS1FfMWRM9v7f3HEu6u+Ynb1Xu68eEfWX4hsmABQYTAAoMJgAUGAwAaDAYAJAgcEEgAKDCQAFBhMACgwmABQYTAAoMJgAUNA8MDDwQT8DAPzG8w0TAAoMJgAUGEwAKDCYAFBgMAGgwGACQIHBBIACgwkABQYTAAqGDfaLD13/qegaoA1DDke/+cDF7Jah1X2dUf+9ndubG77mV/46eogzT/w8eoYxexdF/fR7WqP+8ce/1vCMzbc9EJ3xlit6omc4s2h51Pf94FTU7zj+zYZnXHP/bdEZj229IXqGIcvORv2oqW9H/bZHXm94xoceXhudccSrS7JnWLAn6o+3z476d1/8VsMz3rvooeiMJ3pPR88wumNG1PfckX0WXnvmkYZn/Mrqe6MzfvPC0egZZk2bEvXNXaOi/rX1TzU848c/mr1X9x9siZ5h3Nlsa+bffFvUf/vRf3rfM/qGCQAFBhMACgwmABQYTAAoMJgAUGAwAaDAYAJAgcEEgAKDCQAFBhMACga9Gq97cXZVVuuW7Kq7llXTo/5oZ3Z9UkXb+l9E/fgjS6P+3blPRf2J7uz1K24YdTDqZzbNj/oXN66P+hvvya7iqpi7K7tS8Oykvqifc+XGqO975zNRX3H8iaujfnrfiaifvXtc1O/4ZFvUV5xpHhP1M463R/3GluzPpH9ydgVbxbaxXVE/tis74/43sr9Dbul6I+ortk9ZGPVdW7MtWLKyI+o3to2O+kvxDRMACgwmABQYTAAoMJgAUGAwAaDAYAJAgcEEgAKDCQAFBhMACgwmABQYTAAoGPQu2fNDFkQv1nPyaNSfHD4l6k+sGBn1FWvP3BL1r419PepHhPffLl4wNeorxlw1Iuq79mZ3V87vz+5xbd5yPOordk3ZFfWTJ70Q9Qd6PxL17TPejPqKlR/Nfi7PfXtS1I/9nX1RP7ttRdRXdFxxKOr7F56J+oltWT+p4/Kf8cDE7F7T6SfvzvoFb0f928Mu/xln78n+nGdM+vuoP/3ug1F/Zl12L/ql+IYJAAUGEwAKDCYAFBhMACgwmABQYDABoMBgAkCBwQSAAoMJAAUGEwAKDCYAFAx6l+yhsVdGL9by8Y6oP9iS3Rt57bkxUV/ROWxH1L8zYULUL7j2hqg/cO3eqK842D8v6ntvOBL1kzqzO4SHTsvunq3oWXFd1C/ozl7/+VEHov6u/tuz36Bgz1ULo77v5uFRf2rh5KjvaBsf9RXXfKkv6g/9PLsL+vDc7B7XSeP+Peqbmr7RsJjcenX0imc2ZHdBD5uTfX4Xrbgx6ivGrsw+Lyeb74/6kU3ZGSdumx/1l+IbJgAUGEwAKDCYAFBgMAGgwGACQIHBBIACgwkABQYTAAoMJgAUGEwAKDCYAFDQPDAw8EE/AwD8xvMNEwAKDCYAFBhMACgwmABQYDABoMBgAkDB/wA4i2yh9uNsVgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x288 with 32 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch took 75.967 seconds: d_loss=31.777,  g_loss=-2.998\n",
      "Epoch 2/20\n",
      "Epoch took 73.455 seconds: d_loss=-10.914,  g_loss=-10.991\n",
      "Epoch 3/20\n",
      "Epoch took 74.577 seconds: d_loss=-13.632,  g_loss=0.171\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 32 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch took 75.996 seconds: d_loss=-16.183,  g_loss=19.391\n",
      "Epoch 5/20\n",
      "Epoch took 73.226 seconds: d_loss=-24.256,  g_loss=31.354\n",
      "Epoch 6/20\n",
      "Epoch took 72.593 seconds: d_loss=-21.541,  g_loss=21.442\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 32 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch took 73.522 seconds: d_loss=-17.322,  g_loss=27.330\n",
      "Epoch 8/20\n",
      "Epoch took 74.602 seconds: d_loss=-15.614,  g_loss=27.809\n",
      "Epoch 9/20\n",
      "Epoch took 73.222 seconds: d_loss=-14.814,  g_loss=28.918\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 32 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch took 73.761 seconds: d_loss=-17.116,  g_loss=33.441\n",
      "Epoch 11/20\n"
     ]
    }
   ],
   "source": [
    "train_wgan(ganlist[0][1], dataset, batch_size, codings_size, 4, n_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wgan(ganlist[1][1], dataset, batch_size, codings_size, 8, n_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wgan(ganlist[2][1], dataset, batch_size, codings_size, 16, n_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(d_loss_hist,g_loss_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save gan to files\n",
    "filename = './fish_gan_1.h5'\n",
    "gan.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_path = GIFDIR+\"gan1_{i}.jpg\"\n",
    "for i, imgs in enumerate(imgsetforGIF):\n",
    "    plot_multiple_images(imgs, 8)\n",
    "    plt.savefig(frames_path.format(i=i))\n",
    "constructGIF(len(imgsetforGIF),\"gan1\")\n",
    "display.clear_output()\n",
    "#IpyImage(filename=GIFDIR+\"gan1.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IpyImage(filename=GIFDIR+\"gan1.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "gan = load_model('./fish_gan_1.h5',custom_objects=customOs)\n",
    "generator, discriminator = gan.layers\n",
    "\n",
    "codings_size = 100\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple assessment of proximal images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageDistance(im1,im2):\n",
    "    dist = 0\n",
    "    if len(im1) != len(im2) or len(im1[0]) != len(im2[0]) or len(im1[0][0]) != len(im2[0][0]):\n",
    "        print(\"Images are not the same size\")\n",
    "        return 255. * len(im1) * len(im1[0])* len(im1[0][0])\n",
    "    imx = im1 - im2\n",
    "    return float(tf.math.reduce_sum(imx*imx))**0.5\n",
    "\n",
    "def findClosestImage(genimg,imgset):\n",
    "    distSet = False\n",
    "    bestdist = 10**20\n",
    "    for img in imgset:\n",
    "        curdist = imageDistance(genimg, img)\n",
    "        if curdist < bestdist or not distSet:\n",
    "            bestdist = curdist\n",
    "            bestimg = img\n",
    "            distSet = True\n",
    "    return bestimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = tf.random.normal(shape=[batch_size, 100])\n",
    "generated_images = generator(noise)\n",
    "plot_multiple_images(generated_images, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imID = 1\n",
    "plt.imshow((generated_images[imID]+1)/2)                \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closestim = findClosestImage(generated_images[imID],Xtrain)\n",
    "plt.imshow((closestim+1)/2)                \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
